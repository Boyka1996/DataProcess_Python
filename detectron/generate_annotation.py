# -*- coding: utf-8 -*-
# School                 ：UPC
# Author                 ：Boyka
# E-mail                 ：upcvagen@163.com
# File Name              ：model_assessment_of_single_class.py
# Computer User          ：Administrator
# Current Project        ：DataProcess_Python
# Development Time       ：2020/2/22  15:56
# Development Tool       ：PyCharm
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
import argparse
import logging
import cv2
import numpy as np
import os
import cv2
import xml.etree.ElementTree as ET
import os
from xml.dom import minidom
from collections import defaultdict
import argparse
import cv2  # NOQA (Must import before importing caffe2 due to bug in cv2)
import glob
import logging
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from caffe2.python import workspace

from detectron.core.config import assert_and_infer_cfg
from detectron.core.config import cfg
from detectron.core.config import merge_cfg_from_file
from detectron.utils.io import cache_url
from detectron.utils.logging import setup_logging
from detectron.utils.timer import Timer
import detectron.core.test_engine as infer_engine
import detectron.utils.c2 as c2_utils
import xml.etree.cElementTree as ET
import json
import os

c2_utils.import_detectron_ops()
# OpenCL may be enabled by default in OpenCV3; disable it because it's not
# thread safe and causes unwanted GPU memory allocations.
cv2.ocl.setUseOpenCL(False)
class_precision_dict = {}  # 格式{类别：[真实目标数,检测到的目标数,检测对的目标数,误检目标数,漏检目标数,准确率,召回率]}


def parse_args():
    parser = argparse.ArgumentParser(description='End-to-end inference')
    parser.add_argument(
        '--cfg',
        dest='cfg',
        help='cfg model file (/path/to/model_config.yaml)',
        default='/home/chase/projects/models/hat/e2e_mask_rcnn_R-101-FPN_1x.yaml',
        type=str
    )
    parser.add_argument(
        '--wts',
        dest='weights',
        help='weights model file (/path/to/model_weights.pkl)',
        default='/home/chase/projects/models/hat/model_final.pkl',
        type=str
    )
    parser.add_argument(
        '--src_img',
        dest='src_img',
        help='(/path/to/test/images)',
        default='/home/chase/datasets/safety_helmet/test_images',
        type=str
    )
    parser.add_argument(
        '--tar_xml',
        dest='tar_xml',
        help='The location of xmls',
        default='/home/chase/datasets/safety_helmet/xml',
        type=str
    )
    parser.add_argument(
        '--dataset',
        dest='dataset',
        help='The location of annotations',
        # default=['_background_(这个不要改)', 'belt'],
        default=['_background_(这个不要改)', 'redhat', 'yellowhat', 'bluehat', 'whitehat', 'blackhat', 'greenhat', 'hair',
                 'belt', 'red', 'orange', 'yellow', 'green', 'lightgreen'],
        type=list
    )
    parser.add_argument(
        '--thresh',
        dest='thresh',
        help='交幷比阈值',
        default=0.5,
        type=float
    )
    parser.add_argument(
        '--draw_path',
        dest='draw_path',
        help='画图路径，如果不要画出来那就是None',
        default='/home/chase/projects/models/hat/output',
        type=str
    )

    return parser.parse_args()


def convert_from_cls_format(cls_boxes):
    """Convert from the class boxes/segms/keyps format generated by the testing
    code.
    """
    box_list = [b for b in cls_boxes if len(b) > 0]
    if len(box_list) > 0:
        boxes = np.concatenate(box_list)
    else:
        boxes = None

    classes = []
    for j in range(len(cls_boxes)):
        classes += [j] * len(cls_boxes[j])
    return boxes, classes


def vis_one_image(boxes, thresh=0.7):
    """Visual debugging of detections."""
    result_box = []
    if isinstance(boxes, list):
        boxes, classes = convert_from_cls_format(boxes)
    if boxes is None or boxes.shape[0] == 0 or max(boxes[:, 4]) < thresh:
        return []

    # Display in largest to smallest order to reduce occlusion
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    sorted_inds = np.argsort(-areas)

    for i in sorted_inds:
        bbox = boxes[i, :4]
        score = boxes[i, -1]
        if score < thresh:
            continue
        result_box.append((args.dataset[classes[i]], int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])))
    return result_box


def draw_bboxes(im, bboxes, name):
    RED = (0, 0, 255)
    GREEN = (0, 255, 0)
    COLOR = RED
    if bboxes != []:
        font = cv2.FONT_HERSHEY_SIMPLEX  # 使用默认字体
        for bbox in bboxes:
            cv2.rectangle(im, (bbox[1], bbox[2]), (bbox[3], bbox[4]), COLOR, thickness=2)
            cv2.putText(im, bbox[0], (bbox[1], bbox[2]), font, 0.6, COLOR, 2)
    cv2.imwrite(os.path.join(args.draw_path, name + '.jpg'), im)


def generate_xml(img_name, img_size, points):
    root_element = ET.Element("annotation")

    folder = ET.SubElement(root_element, "folder")
    folder.text = "UPC"
    filename = ET.SubElement(root_element, "filename")
    filename.text = img_name
    path = ET.SubElement(root_element, "path")
    path.text = "None"
    source = ET.SubElement(root_element, "source")
    database = ET.SubElement(source, "database")
    database.text = "Unknown"
    size = ET.SubElement(root_element, "size")

    width = ET.SubElement(size, "width")
    height = ET.SubElement(size, "height")
    depth = ET.SubElement(size, "depth")
    width.text = str(img_size[0])
    height.text = str(img_size[1])
    depth.text = "3"
    segmented = ET.SubElement(root_element, "segmented")
    segmented.text = "0"

    for point_id, point in enumerate(points):
        obj = ET.Element("object")
        name = ET.SubElement(obj, "name")
        pose = ET.SubElement(obj, "pose")
        truncated = ET.SubElement(obj, "truncated")
        difficult = ET.SubElement(obj, "difficult")
        name.text = point[0]
        pose.text = "Unspecified"
        truncated.text = "0"
        difficult.text = "Unknown"

        bndbox = ET.SubElement(obj, "bndbox")
        xmin = ET.SubElement(bndbox, "xmin")
        ymin = ET.SubElement(bndbox, "ymin")
        xmax = ET.SubElement(bndbox, "xmax")
        ymax = ET.SubElement(bndbox, "ymax")
        xmin.text = str(point[2])
        ymin.text = str(point[3])
        xmax.text = str(point[4])
        ymax.text = str(point[5])
        root_element.append(obj)
    # 创建xml树，并将根节点放入其中
    xml_string = ET.tostring(root_element)

    dom = minidom.parseString(xml_string)
    with open(os.path.join(args.tar_xml, img_name.replace(".jpg", ".xml")), 'w', encoding='utf-8') as f:
        # indent为根节点缩进，newl每行数据句末符号，addindent为其他节点缩进
        dom.writexml(f, indent='\t', newl='\n',
                     addindent='\t', encoding='utf-8')


def main(args):
    logger = logging.getLogger(__name__)
    merge_cfg_from_file(args.cfg)
    cfg.TEST.WEIGHTS = args.weights

    cfg.NUM_GPUS = 1
    assert_and_infer_cfg()
    model = infer_engine.initialize_model_from_cfg(args.weights)
    for image in os.listdir(args.srcImg):
        try:
            im = cv2.imread(os.path.join(args.srcImg, image))
            with c2_utils.NamedCudaScope(0):
                cls_boxes, cls_segms, cls_keyps = infer_engine.im_detect_all(
                    model, im, None, None
                )
                bboxes = vis_one_image(cls_boxes, args.thresh)
                print(bboxes)
        except:
            print('获取检测结果的时候歇逼了')
        try:
            if args.draw_path != None:
                if not os.path.exists(args.draw_path):
                    os.makedirs(args.draw_path)
                draw_bboxes(im, bboxes, image.split('.')[0])
            if args.tar_xml != None:
                if not os.path.exists(args.tar_xml):
                    os.makedirs(args.tar_xml)
                generate_xml(image, (im.shape[0], im.shape[1]), bboxes)
        except:
            print("error")


if __name__ == '__main__':
    workspace.GlobalInit(['caffe2', '--caffe2_log_level=0'])
    setup_logging(__name__)
    args = parse_args()
    main(args)
